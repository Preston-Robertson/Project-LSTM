{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46969b88",
   "metadata": {},
   "source": [
    "# IE 8990 Custom LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb1aab",
   "metadata": {},
   "source": [
    "## Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914a444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries ########################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8872a2",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cdb5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Harry Potter Dataset\n",
    "\n",
    "data = open('HP1.txt').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d623f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 439742 characters, 53 unique\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "\n",
    "chars = set(data)\n",
    "vocab_size = len(chars)\n",
    "print('data has %d characters, %d unique' % (len(data), vocab_size))\n",
    "\n",
    "char_to_idx = {w: i for i,w in enumerate(chars)}\n",
    "idx_to_char = {i: w for i,w in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09bd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce9066e1",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e392a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "065f2c4c",
   "metadata": {},
   "source": [
    "## Creating the Custom LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eedd5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the LSTM Class\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, char_to_idx, idx_to_char, vocab_size, n_h=100, seq_len=25, \n",
    "                          epochs=10, lr=0.01, beta1=0.9, beta2=0.999):\n",
    "        self.char_to_idx = char_to_idx # characters to indices mapping\n",
    "        self.idx_to_char = idx_to_char # indices to characters mapping\n",
    "        self.vocab_size = vocab_size # no. of unique characters in the training data\n",
    "        self.n_h = n_h # no. of units in the hidden layer\n",
    "        self.seq_len = seq_len # no. of time steps, also size of mini batch\n",
    "        self.epochs = epochs # no. of training iterations\n",
    "        self.lr = lr # learning rate\n",
    "        self.beta1 = beta1 # 1st momentum parameter\n",
    "        self.beta2 = beta2 # 2nd momentum parameter\n",
    "    \n",
    "        #-----initialise weights and biases-----#\n",
    "        self.params = {}\n",
    "        std = (1.0/np.sqrt(self.vocab_size + self.n_h)) # Xavier initialisation\n",
    "        \n",
    "        # forget gate\n",
    "        self.params[\"Wf\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bf\"] = np.ones((self.n_h,1))\n",
    "\n",
    "        # input gate\n",
    "        self.params[\"Wi\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bi\"] = np.zeros((self.n_h,1))\n",
    "\n",
    "        # cell gate\n",
    "        self.params[\"Wc\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bc\"] = np.zeros((self.n_h,1))\n",
    "\n",
    "        # output gate\n",
    "        self.params[\"Wo\"] = np.random.randn(self.n_h, self.n_h + self.vocab_size) * std\n",
    "        self.params[\"bo\"] = np.zeros((self.n_h ,1))\n",
    "\n",
    "        # output\n",
    "        self.params[\"Wv\"] = np.random.randn(self.vocab_size, self.n_h) * \\\n",
    "                                          (1.0/np.sqrt(self.vocab_size))\n",
    "        self.params[\"bv\"] = np.zeros((self.vocab_size ,1))\n",
    "\n",
    "        #-----initialise gradients and Adam parameters-----#\n",
    "        self.grads = {}\n",
    "        self.adam_params = {}\n",
    "\n",
    "        for key in self.params:\n",
    "            self.grads[\"d\"+key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"m\"+key] = np.zeros_like(self.params[key])\n",
    "            self.adam_params[\"v\"+key] = np.zeros_like(self.params[key])\n",
    "            \n",
    "        self.smooth_loss = -np.log(1.0 / self.vocab_size) * self.seq_len\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c7f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Activation Functions\n",
    "\n",
    "\n",
    "def sigmoid(self, x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "LSTM.sigmoid = sigmoid\n",
    "\n",
    "\n",
    "def softmax(self, x):\n",
    "    e_x = np.exp(x - np.max(x)) # max(x) subtracted for numerical stability\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "LSTM.softmax = softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a314869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for fixing gradients\n",
    "\n",
    "\n",
    "def clip_grads(self):\n",
    "    for key in self.grads:\n",
    "        np.clip(self.grads[key], -5, 5, out=self.grads[key])\n",
    "    return\n",
    "\n",
    "LSTM.clip_grads = clip_grads\n",
    "\n",
    "\n",
    "def reset_grads(self):\n",
    "    for key in self.grads:\n",
    "        self.grads[key].fill(0)\n",
    "    return\n",
    "\n",
    "LSTM.reset_grads = reset_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388765d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the parameters through Adam\n",
    "\n",
    "\n",
    "def update_params(self, batch_num):\n",
    "    for key in self.params:\n",
    "        self.adam_params[\"m\"+key] = self.adam_params[\"m\"+key] * self.beta1 + \\\n",
    "                                    (1 - self.beta1) * self.grads[\"d\"+key]\n",
    "        self.adam_params[\"v\"+key] = self.adam_params[\"v\"+key] * self.beta2 + \\\n",
    "                                    (1 - self.beta2) * self.grads[\"d\"+key]**2\n",
    "\n",
    "        m_correlated = self.adam_params[\"m\" + key] / (1 - self.beta1**batch_num)\n",
    "        v_correlated = self.adam_params[\"v\" + key] / (1 - self.beta2**batch_num) \n",
    "        self.params[key] -= self.lr * m_correlated / (np.sqrt(v_correlated) + 1e-8) \n",
    "    return\n",
    "\n",
    "LSTM.update_params = update_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7828fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward step through the LSTM models\n",
    "\n",
    "\n",
    "def forward_step(self, x, h_prev, c_prev):\n",
    "    z = np.row_stack((h_prev, x))\n",
    "\n",
    "    f = self.sigmoid(np.dot(self.params[\"Wf\"], z) + self.params[\"bf\"])\n",
    "    i = self.sigmoid(np.dot(self.params[\"Wi\"], z) + self.params[\"bi\"])\n",
    "    c_bar = np.tanh(np.dot(self.params[\"Wc\"], z) + self.params[\"bc\"])\n",
    "\n",
    "    c = f * c_prev + i * c_bar\n",
    "    o = self.sigmoid(np.dot(self.params[\"Wo\"], z) + self.params[\"bo\"])\n",
    "    h = o * np.tanh(c)\n",
    "\n",
    "    v = np.dot(self.params[\"Wv\"], h) + self.params[\"bv\"]\n",
    "    y_hat = self.softmax(v)\n",
    "    return y_hat, v, h, o, c, c_bar, i, f, z\n",
    "\n",
    "LSTM.forward_step = forward_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56d3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward step for Back propagation\n",
    "\n",
    "\n",
    "\n",
    "def backward_step(self, y, y_hat, dh_next, dc_next, c_prev, z, f, i, c_bar, c, o, h):\n",
    "    dv = np.copy(y_hat)\n",
    "    dv[y] -= 1 # yhat - y\n",
    "\n",
    "    self.grads[\"dWv\"] += np.dot(dv, h.T)\n",
    "    self.grads[\"dbv\"] += dv\n",
    "\n",
    "    dh = np.dot(self.params[\"Wv\"].T, dv)\n",
    "    dh += dh_next\n",
    "    \n",
    "    do = dh * np.tanh(c)\n",
    "    da_o = do * o*(1-o)\n",
    "    self.grads[\"dWo\"] += np.dot(da_o, z.T)\n",
    "    self.grads[\"dbo\"] += da_o\n",
    "\n",
    "    dc = dh * o * (1-np.tanh(c)**2)\n",
    "    dc += dc_next\n",
    "\n",
    "    dc_bar = dc * i\n",
    "    da_c = dc_bar * (1-c_bar**2)\n",
    "    self.grads[\"dWc\"] += np.dot(da_c, z.T)\n",
    "    self.grads[\"dbc\"] += da_c\n",
    "\n",
    "    di = dc * c_bar\n",
    "    da_i = di * i*(1-i) \n",
    "    self.grads[\"dWi\"] += np.dot(da_i, z.T)\n",
    "    self.grads[\"dbi\"] += da_i\n",
    "\n",
    "    df = dc * c_prev\n",
    "    da_f = df * f*(1-f)\n",
    "    self.grads[\"dWf\"] += np.dot(da_f, z.T)\n",
    "    self.grads[\"dbf\"] += da_f\n",
    "\n",
    "    dz = (np.dot(self.params[\"Wf\"].T, da_f)\n",
    "         + np.dot(self.params[\"Wi\"].T, da_i)\n",
    "         + np.dot(self.params[\"Wc\"].T, da_c)\n",
    "         + np.dot(self.params[\"Wo\"].T, da_o))\n",
    "\n",
    "    dh_prev = dz[:self.n_h, :]\n",
    "    dc_prev = f * dc\n",
    "    return dh_prev, dc_prev\n",
    "\n",
    "LSTM.backward_step = backward_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3b268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the forward and backward passes are implemented\n",
    "\n",
    "\n",
    "def forward_backward(self, x_batch, y_batch, h_prev, c_prev):\n",
    "    x, z = {}, {}\n",
    "    f, i, c_bar, c, o = {}, {}, {}, {}, {}\n",
    "    y_hat, v, h = {}, {}, {}\n",
    "\n",
    "    # Values at t= - 1\n",
    "    h[-1] = h_prev\n",
    "    c[-1] = c_prev\n",
    "\n",
    "    loss = 0\n",
    "    for t in range(self.seq_len): \n",
    "        x[t] = np.zeros((self.vocab_size, 1))\n",
    "        x[t][x_batch[t]] = 1\n",
    "\n",
    "        y_hat[t], v[t], h[t], o[t], c[t], c_bar[t], i[t], f[t], z[t] = \\\n",
    "        self.forward_step(x[t], h[t-1], c[t-1])\n",
    "\n",
    "        loss += -np.log(y_hat[t][y_batch[t],0])\n",
    "\n",
    "    self.reset_grads()\n",
    "\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dc_next = np.zeros_like(c[0])\n",
    "\n",
    "    for t in reversed(range(self.seq_len)):\n",
    "        dh_next, dc_next = self.backward_step(y_batch[t], y_hat[t], dh_next, \n",
    "                                              dc_next, c[t-1], z[t], f[t], i[t], \n",
    "                                              c_bar[t], c[t], o[t], h[t]) \n",
    "    return loss, h[self.seq_len-1], c[self.seq_len-1]\n",
    "\n",
    "LSTM.forward_backward = forward_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d412241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harry Potter Dataset Specific\n",
    "\n",
    "\n",
    "def sample(self, h_prev, c_prev, sample_size):\n",
    "    x = np.zeros((self.vocab_size, 1))\n",
    "    h = h_prev\n",
    "    c = c_prev\n",
    "    sample_string = \"\" \n",
    "    \n",
    "    for t in range(sample_size):\n",
    "        y_hat, _, h, _, c, _, _, _, _ = self.forward_step(x, h, c)        \n",
    "        \n",
    "        # get a random index within the probability distribution of y_hat(ravel())\n",
    "        idx = np.random.choice(range(self.vocab_size), p=y_hat.ravel())\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        #find the char with the sampled index and concat to the output string\n",
    "        char = self.idx_to_char[idx]\n",
    "        sample_string += char\n",
    "    return sample_string\n",
    "\n",
    "LSTM.sample = sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4f800",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fab9a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Train function\n",
    "\n",
    "\n",
    "def train(self, X, verbose=True):\n",
    "    J = []  # to store losses\n",
    "\n",
    "    num_batches = len(X) // self.seq_len\n",
    "    X_trimmed = X[: num_batches * self.seq_len]  # trim input to have full sequences\n",
    "\n",
    "    for epoch in range(self.epochs):\n",
    "        h_prev = np.zeros((self.n_h, 1))\n",
    "        c_prev = np.zeros((self.n_h, 1))\n",
    "\n",
    "        for j in range(0, len(X_trimmed) - self.seq_len, self.seq_len):\n",
    "            # prepare batches\n",
    "            x_batch = [self.char_to_idx[ch] for ch in X_trimmed[j: j + self.seq_len]]\n",
    "            y_batch = [self.char_to_idx[ch] for ch in X_trimmed[j + 1: j + self.seq_len + 1]]\n",
    "\n",
    "            loss, h_prev, c_prev = self.forward_backward(x_batch, y_batch, h_prev, c_prev)\n",
    "\n",
    "            # smooth out loss and store in list\n",
    "            self.smooth_loss = self.smooth_loss * 0.999 + loss * 0.001\n",
    "            J.append(self.smooth_loss)\n",
    "\n",
    "            # check gradients\n",
    "            if epoch == 0 and j == 0:\n",
    "                self.gradient_check(x_batch, y_batch, h_prev, c_prev, num_checks=10, \n",
    "                                    delta=1e-7)\n",
    "\n",
    "            self.clip_grads()\n",
    "\n",
    "            batch_num = epoch * self.epochs + j / self.seq_len + 1\n",
    "            self.update_params(batch_num)\n",
    "\n",
    "            # print out loss and sample string\n",
    "            if verbose:\n",
    "                if j % 400000 == 0:\n",
    "                    print('Epoch:', epoch, '\\tBatch:', j, \"-\", j + self.seq_len,\n",
    "                          '\\tLoss:', round(self.smooth_loss, 2))\n",
    "                    s = self.sample(h_prev, c_prev, sample_size=250)\n",
    "                    print(s, \"\\n\")\n",
    "    return J, self.params\n",
    "\n",
    "LSTM.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4eae978",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTM' object has no attribute 'gradient_check'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PRESTO~1\\AppData\\Local\\Temp/ipykernel_45284/1260562272.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_char\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mJ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\PRESTO~1\\AppData\\Local\\Temp/ipykernel_45284/2144698371.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, verbose)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;31m# check gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 self.gradient_check(x_batch, y_batch, h_prev, c_prev, num_checks=10, \n\u001b[0m\u001b[0;32m     28\u001b[0m                                     delta=1e-7)\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LSTM' object has no attribute 'gradient_check'"
     ]
    }
   ],
   "source": [
    "# Fitting the Model to Data\n",
    "\n",
    "\n",
    "model = LSTM(char_to_idx, idx_to_char, vocab_size, epochs = 5, lr = 0.01)\n",
    "\n",
    "J, params = model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697aec56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa8d42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
